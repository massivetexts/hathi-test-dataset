{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare JSON and Parquet EF representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import Volume, utils\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv('../test_dataset_htids.csv.gz', names=['htid'])['htid']\n",
    "jsonpaths = ids.apply(lambda x: '/data/extracted-features/' + utils.id_to_rsync(x))\n",
    "parqpaths = ids.apply(lambda x: '/data/extracted-features-parquet/' + utils.id_to_rsync(x)).str.replace('.json.bz2', '')\n",
    "parqchunkpaths = parqpaths.str.replace('extracted-features-parquet', 'extracted-features-parquet-chunked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_if(path):\n",
    "    try:\n",
    "        return os.stat(path).st_size\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.98, 0.14, 32.1, 32.24, 22.9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In GB\n",
    "jsonsize = jsonpaths.apply(stat_if).div(1024**3).sum()\n",
    "metasize = (parqpaths + '.meta.json').apply(stat_if).div(1024**3).sum()\n",
    "parqsize = (parqpaths + '.tokens.parquet').apply(stat_if).div(1024**3).sum()\n",
    "parqchunksize = (parqchunkpaths + '.tokens.parquet').apply(stat_if).div(1024**3).sum()\n",
    "jsonsize.round(2), metasize.round(2), parqsize.round(2), (metasize+parqsize).round(2), (metasize+parqchunksize).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet is larger by 134%\n"
     ]
    }
   ],
   "source": [
    "print(\"Parquet is larger by {}%\".format(int((metasize+parqsize)/jsonsize*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked parquet (5000 words/chunk) is 95% of the JSON size\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunked parquet (5000 words/chunk) is {}% of the JSON size\".format(int((metasize+parqchunksize)/jsonsize*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on token loading\n",
    "\n",
    "As you would expect, the Parquet option is much quicker. Some notes, though:\n",
    "\n",
    "- the parquet option is not only reading parquet files, but their associated metadata file in JSON. It's possible to save without the metadata, but it's small enough.\n",
    "- Of course it's quicker! In addition to not needing JSON parsing and using faster decompression than BZIP2, the data has already been preprocessed and formatted into a table format.\n",
    "\n",
    "The point is that if you ever expect to read your files *more than once*, [converting your local Extracted Features collection to parquet](https://github.com/massivetexts/compare-tools/blob/master/scripts/convert-to-parquet.py) using the `Volume.save_parquet` function will save you a great deal of computing time. It is also processing that can be front-loaded - converting to Parquet can be done in the background while you're developing your project code, not at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 9s, sys: 5.64 s, total: 4min 15s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for path in jsonpaths.head(1000):\n",
    "    vol = Volume(path, parser='json')\n",
    "    tl = vol.tokenlist(pos=False, case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.6 s, sys: 587 ms, total: 54.2 s\n",
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for path in parqpaths.head(1000):\n",
    "    vol = Volume(path, parser='parquet')\n",
    "    tl = vol.tokenlist(pos=False, case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 17s, sys: 7.7 s, total: 9min 25s\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for path in jsonpaths.head(1000):\n",
    "    vol = Volume(path, parser='json')\n",
    "    tl = vol.chunked_tokenlist(chunk_target=5000, pos=False, case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 42s, sys: 3.06 s, total: 5min 45s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for path in parqpaths.head(1000):\n",
    "    vol = Volume(path, parser='parquet')\n",
    "    tl = vol.chunked_tokenlist(chunk_target=5000, pos=False, case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 s, sys: 487 ms, total: 34.3 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for path in parqchunkpaths.head(1000):\n",
    "    vol = Volume(path, parser='parquet')\n",
    "    tl = vol.chunked_tokenlist(pos=False, case=False, suppress_warning=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
